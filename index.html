<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="950"/>
  <meta property="og:image:height" content="540"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>The Devil is in Temporal Token: High Quality Video Reasoning Segmentation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">The Devil is in Temporal Token: High Quality Video Reasoning Segmentation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/SitongGong" target="_blank">Sitong Gong</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Yunzhi Zhuge</a><sup>1*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Lu Zhang</a><sup>1</sup>,</span>
                    <span class="author-block">
                      <a href="https://github.com/z-x-yang" target="_blank">Zongxin Yang</a><sup>2</sup>,</span>
                      <span class="author-block">
                        <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Pingping Zhang</a><sup>1</sup>,</span>
                        <span class="author-block">
                          <a href="https://github.com/huchuanlu" target="_blank">Huchuan Lu</a><sup>1</sup>,</span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Dalian University of Technology<sup>1</sup>, Harvard University<sup>2</sup>
                      <br>CVPR 2025</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Auther</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2501.08549" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/SitongGong/VRS-HQ" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
      <h2 class="title is-3">In-the-wild Video Demo</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/mPU6b2pt.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
          A prominent figure subtly highlighted within a commercial promotional video, whose presence and actions serve as the central point for engagement and communication. 
          </h2>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/1vRmoACZ.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Who displays the most dynamic and expressive range of movement during the dance, transitioning seamlessly between sharp, high-energy motions, captivating attention with their vibrant and energetic performance. 
          </h2>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/watch.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            An item used for timekeeping in daily life and offering aesthetic appeal, commonly worn on the wrist and often featuring a circular or rectangular design, occasionally appearing in the video.
          </h2>
        </div>
        <div class="item item-video4">
          <video poster="" id="video4" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/S1_Cheese_C1.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            A white object with a circular smooth surface, typically placed on a table and unmoved by the videographer, designed to hold and present various types of food, offering functionality in dining settings.
          </h2>
        </div>
      </div>
    </div>
  </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
        Existing methods for Video Reasoning Segmentation rely heavily on a single special token to represent the object in the keyframe or the entire video, 
        inadequately capturing spatial complexity and inter-frame motion. To overcome these challenges, we propose VRS-HQ, an end-to-end video reasoning segmentation approach that leverages Multimodal Large Language Models (MLLMs) to inject rich spatiotemporal features
        into hierarchical tokens. Our key innovations include a Temporal Dynamic Aggregation (TDA) and a Token-driven Keyframe Selection (TKS). Specifically, we design frame-level &lt;SEG&gt; and temporal-level &lt;TAK&gt; tokens that utilize MLLM's autoregressive learning to effectively capture both local and global information.
        Subsequently, we apply a similarity-based weighted fusion and frame selection strategy, then utilize SAM2 to perform keyframe segmentation and propagation. To enhance keyframe localization accuracy, the TKS filters keyframes based on SAM2's occlusion scores during inference. VRS-HQ achieves state-of-the-art performance on ReVOS, 
        surpassing VISA by 5.9%/12.5%/9.1% in J&F scores across the three subsets. These results highlight the strong temporal reasoning and segmentation capabilities of our method.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Teasor -->
<!-- <section class="hero is-small is-light"> -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Comparison with Previous Methods</h2>
          <div class="item">
            <!-- Your image here -->
            <img src="static/CVPR_html/teasor.png" alt="MY ALT TEXT"/>
            <h2 class="subtitle has-text-centered">
              Comparison with previous VRS approaches. 
              (a) Previous methods utilize a single &lt;SEG&gt; token for keyframe-based segmentation, depending heavily on external models for keyframe detection and mask propagation. This reliance can hinder accurate keyframe localization and prevent end-to-end inference. 
              (b) In contrast, VRS-HQ introduces frame-level &lt;SEG&gt; and a temporal &lt;TAK&gt; token for dynamic aggregation. The aggregated &lt;TAK&gt; token is then used for both keyframe selection and mask generation within SAM2. This enables single-stage inference with precise keyframe selection and high-quality segmentation. 
              (c) VRS-HQ achieves state-of-the-art per- formance on various image and video datasets across reasoning and referring segmentation.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Teasor -->
<!-- <section class="hero is-small is-light"> -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-3">Overall Framework of VRS-HQ</h2>
          <div class="item">
            <!-- Your image here -->
            <img src="static/CVPR_html/pipeline.png" alt="MY ALT TEXT"/>
            <h2 class="subtitle has-text-centered">
            (a) VRS-HQ architecture. VRS-HQ incorporates a Multimodal Large Language Model for Temporal Token Encoding (&lt;SEG&gt; and &lt;TAK&gt; tokens, §3.1), a Temporal Dynamic Aggregation, a Token-driven Keyframe Selection and Mask Decoding and Propogation. 
            (b) Temporal Dynamic Aggregation (TDA) merges frame-level &lt;SEG&gt; tokens into a temporal &lt;TAK&gt; token using a weighted fusion based on cosine similarity. (§3.2). 
            (c) Token-driven Keyframe Selection (TKS). During training, the frame with the &lt;SEG&gt; token closest to the &lt;TAK&gt; token is selected as the keyframe. During inference, keyframe selection is refined using SAM2's occlusion scores and token similarity scores (§3.3). 
            (d) Mask Decoding and Propagation (MDP). The &lt;TAK&gt; token provides a sparse embedding for SAM2, generating a keyframe mask and propagating it to other frames via a memory mechanism (§3.4).
            </h2>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->



<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Comparison of VRS-HQ and VISA in Various Scenarios on the ReVOS Benchmark</h2>

      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/CVPR_html/visualization1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        Qualitative comparison of VRS-HQ and VISA in explicit language-based referring scenarios on the ReVOS benchmark.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/CVPR_html/visualization2.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        Qualitative comparison of VRS-HQ and VISA in scenarios incorporating complex temporal dynamics on the ReVOS benchmark.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/CVPR_html/visualization3.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        Qualitative comparison of VRS-HQ and VISA in reasoning scenarios that require world knowledge on the ReVOS benchmark.
       </h2>
     </div>
     <!-- <div class="item"> -->
      <!-- Your image here -->
      <!-- <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2> -->
    <!-- </div> -->
  </div>
</div>
</div>
</section>
<!-- End image carousel -->











<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <!-- <h2 class="title">BibTeX</h2> -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <h2 class="title is-3">BibTeX</h2>
      <pre><code>@article{gong2025devil,
        title={The Devil is in Temporal Token: High Quality Video Reasoning Segmentation},
        author={Gong, Sitong and Zhuge, Yunzhi and Zhang, Lu and Yang, Zongxin and Zhang, Pingping and Lu, Huchuan},
        journal={arXiv preprint arXiv:2501.08549},
        year={2025}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
